# trainGPT-2
My learning journey to move from understanding the theory of Transformer models to "almost seriously" training my own GPT-2 (124M parameters) from scratch. "Almost serious" means: using proper datasets, understanding hyperparameters, monitoring metrics, and likely using a real GPU rather than just running a demo script. 
